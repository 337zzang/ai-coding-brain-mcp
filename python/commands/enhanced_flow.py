#!/usr/bin/env python3
"""
Enhanced flow command with integrated file structure management and complete briefing
"""

import os
import sys
import json
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from pathlib import Path

# helpers ì „ì—­ ë³€ìˆ˜ ì ‘ê·¼
helpers = globals().get('helpers', None)

# ê¸°ë³¸ imports
import sys
import os

# Python ê²½ë¡œ ì„¤ì • - ìƒëŒ€ import ë¬¸ì œ í•´ê²°
python_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if python_path not in sys.path:
    sys.path.insert(0, python_path)

# ì´ì œ ì ˆëŒ€ import ì‚¬ìš©
from core.context_manager import get_context_manager, initialize_context
from core.config import get_project_path
from smart_print import smart_print
from analyzers.project_analyzer import ProjectAnalyzer
from project_briefing import print_project_briefing



# ìºì‹± ì„¤ì •
CACHE_EXPIRY_HOURS = 24  # file_directory.md ìºì‹œ ìœ íš¨ ì‹œê°„
MAX_CACHE_SIZE_MB = 100  # ìµœëŒ€ ìºì‹œ í¬ê¸°
PERFORMANCE_TRACKING = True  # ì„±ëŠ¥ ì¶”ì  í™œì„±í™”

def debug_log(message):
    """ë””ë²„ê¹… ë©”ì‹œì§€ë¥¼ íŒŒì¼ì— ê¸°ë¡"""
    import datetime
    with open("debug_flow.log", "a", encoding="utf-8") as f:
        f.write(f"[{datetime.datetime.now()}] {message}\n")
        f.flush()

    def __init__(self, project_path: str, helpers=None):
        self.project_path = project_path
        self.helpers = helpers or globals().get('helpers', None)
        self.structure_cache = {}
        self.file_metadata = {}
        
    def scan_project_structure(self) -> Dict[str, Any]:
        """í”„ë¡œì íŠ¸ êµ¬ì¡°ë¥¼ ìŠ¤ìº”í•˜ê³  ë¶„ì„ (í•µì‹¬ íŒŒì¼ í•„í„°ë§ í¬í•¨)"""
        
        self.core_paths = ['src/', 'python/', 'memory/']
        self.exclude_paths = [
            'vendor/', 'backups/', 'test/', '__tests__/', 'tests/',
            '.git/', 'node_modules/', '__pycache__/', 'dist/', 
            'build/', '.next/', '.cache/', 'coverage/', 'venv/', 
            'env/', '.env/', '.idea/', '.vscode/'
        ]
        self.core_extensions = {'.ts', '.js', '.py', '.json', '.md', '.yml', '.yaml'}
        self.exclude_extensions = {'.bak', '.log', '.tmp', '.cache', '.lock', '.resolved'}
        
        structure = {
            "files": {},
            "directories": {},
            "statistics": {
                "total_files": 0,
                "core_files": 0,
                "excluded_files": 0,
                "by_extension": {},
                "by_directory": {},
                "large_files": [],
                "recent_files": []
            },
            "key_files": {
                "entry_points": [],
                "configs": [],
                "tests": [],
                "docs": []
            }
        }
        
        def is_core_file(rel_path: str, file_name: str) -> bool:
            """í•µì‹¬ íŒŒì¼ì¸ì§€ í™•ì¸"""
            # ì œì™¸ ê²½ë¡œ ì²´í¬
            path_parts = rel_path.replace('\\', '/').lower()
            if any(exc in path_parts for exc in self.exclude_paths):
                return False
            
            # ì œì™¸ í™•ì¥ì ì²´í¬
            if any(file_name.endswith(ext) for ext in self.exclude_extensions):
                return False
            
            # ìˆ¨ê¹€ íŒŒì¼/í´ë” ì œì™¸
            if any(part.startswith('.') for part in rel_path.split(os.sep)):
                return False
            
            # í•µì‹¬ ê²½ë¡œ ì²´í¬
            in_core_path = any(core in rel_path.replace('\\', '/') for core in self.core_paths)
            
            # í•µì‹¬ í™•ì¥ì ì²´í¬
            ext = Path(file_name).suffix
            has_core_ext = ext in self.core_extensions
            
            # ë£¨íŠ¸ ì„¤ì • íŒŒì¼ ì²´í¬
            is_root_config = rel_path.count(os.sep) <= 1 and ext in {'.json', '.yml', '.yaml', '.toml'}
            
            return (in_core_path and has_core_ext) or is_root_config
        
        # ë””ë ‰í† ë¦¬ ìˆœíšŒ
        for root, dirs, files in os.walk(self.project_path):
            rel_root = os.path.relpath(root, self.project_path)
            
            # ì œì™¸í•  ë””ë ‰í† ë¦¬ í•„í„°ë§
            dirs[:] = [d for d in dirs if not d.startswith('.') and 
                      d not in {'node_modules', '__pycache__', 'dist', 'build', 
                               '.next', 'venv', 'env', 'coverage', 'backups'}]
            
            structure["directories"][rel_root] = {
                "file_count": len(files),
                "subdirs": dirs
            }
            
            for file in files:
                file_path = os.path.join(root, file)
                rel_path = os.path.relpath(file_path, self.project_path)
                
                try:
                    stat = os.stat(file_path)
                    file_info = {
                        "size": stat.st_size,
                        "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                        "extension": Path(file).suffix,
                        "is_core": is_core_file(rel_path, file)
                    }
                    
                    structure["files"][rel_path] = file_info
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    structure["statistics"]["total_files"] += 1
                    if file_info["is_core"]:
                        structure["statistics"]["core_files"] += 1
                    else:
                        structure["statistics"]["excluded_files"] += 1
                    
                    ext = Path(file).suffix
                    structure["statistics"]["by_extension"][ext] = \
                        structure["statistics"]["by_extension"].get(ext, 0) + 1
                    
                    # í•µì‹¬ íŒŒì¼ë§Œ ì¶”ê°€ ë¶„ì„
                    if file_info["is_core"]:
                        # í° íŒŒì¼ ì¶”ì  (100KB ì´ìƒ)
                        if stat.st_size > 100000:
                            structure["statistics"]["large_files"].append({
                                "path": rel_path,
                                "size": stat.st_size
                            })
                        
                        # ì£¼ìš” íŒŒì¼ ë¶„ë¥˜
                        if file in ['index.py', 'main.py', 'app.py', 'index.ts', 'main.ts']:
                            structure["key_files"]["entry_points"].append(rel_path)
                        elif file.endswith(('.json', '.yaml', '.yml', '.toml')) and rel_path.count(os.sep) <= 1:
                            structure["key_files"]["configs"].append(rel_path)
                        elif 'test' in file.lower() or 'spec' in file.lower():
                            structure["key_files"]["tests"].append(rel_path)
                        elif file.endswith('.md'):
                            structure["key_files"]["docs"].append(rel_path)
                        
                except Exception as e:
                    structure["files"][rel_path] = {"error": str(e)}
        
        # ìµœê·¼ ìˆ˜ì •ëœ í•µì‹¬ íŒŒì¼ (ìƒìœ„ 10ê°œ)
        recent_files = sorted(
            [(p, f["modified"]) for p, f in structure["files"].items() 
             if "modified" in f and f.get("is_core", False)],
            key=lambda x: x[1],
            reverse=True
        )[:10]
        
        structure["statistics"]["recent_files"] = [
            {"path": p, "modified": m} for p, m in recent_files
        ]
        
        self.structure_cache = structure
        return structure

    def scan_directory(self) -> Dict[str, Any]:
        """scan_project_structureì˜ ë³„ì¹­ (í˜¸í™˜ì„±ì„ ìœ„í•´)"""
        return self.scan_project_structure()

    def get_file_context(self, file_path: str) -> Dict[str, Any]:
        """íŠ¹ì • íŒŒì¼ì˜ context ì •ë³´ ë°˜í™˜"""
        rel_path = os.path.relpath(file_path, self.project_path)
        
        if rel_path in self.structure_cache.get("files", {}):
            file_info = self.structure_cache["files"][rel_path].copy()
            
            # ì¶”ê°€ context ì •ë³´
            file_info["directory"] = os.path.dirname(rel_path)
            file_info["filename"] = os.path.basename(rel_path)
            
            # ê°™ì€ ë””ë ‰í† ë¦¬ì˜ ê´€ë ¨ íŒŒì¼ë“¤
            same_dir_files = [
                f for f in self.structure_cache["files"]
                if os.path.dirname(f) == file_info["directory"] and f != rel_path
            ]
            file_info["related_files"] = same_dir_files[:5]
            
            return file_info
        
        return {}
    
    def update_file_access(self, file_path: str):
        """íŒŒì¼ ì ‘ê·¼ ì‹œ metadata ì—…ë°ì´íŠ¸"""
        rel_path = os.path.relpath(file_path, self.project_path)
        if rel_path not in self.file_metadata:
            self.file_metadata[rel_path] = {
                "access_count": 0,
                "last_accessed": None,
                "modifications": []
            }
        
        self.file_metadata[rel_path]["access_count"] += 1
        self.file_metadata[rel_path]["last_accessed"] = datetime.now().isoformat()



    def get_core_files(self) -> List[Dict[str, Any]]:
        """í”„ë¡œì íŠ¸ í•µì‹¬ íŒŒì¼ë§Œ ê°€ì ¸ì˜¤ê¸°"""
        if not self.structure_cache:
            self.scan_project_structure()
        
        core_files = []
        for path, info in self.structure_cache.get("files", {}).items():
            if info.get("is_core", False):
                core_files.append({
                    "path": path,
                    "name": os.path.basename(path),
                    "type": "file",
                    "size": info.get("size", 0),
                    "modified": info.get("modified", ""),
                    "extension": info.get("extension", "")
                })
        
        return core_files
    
    def search_core_files(self, pattern: str) -> List[Dict[str, Any]]:
        """í•µì‹¬ íŒŒì¼ì—ì„œë§Œ ê²€ìƒ‰"""
        if not self.structure_cache:
            self.scan_project_structure()
        
        pattern_lower = pattern.lower()
        results = []
        
        for path, info in self.structure_cache.get("files", {}).items():
            if info.get("is_core", False) and pattern_lower in path.lower():
                results.append({
                    "path": path,
                    "name": os.path.basename(path),
                    "type": "file",
                    "size": info.get("size", 0),
                    "modified": info.get("modified", ""),
                    "extension": info.get("extension", "")
                })
        
        return results
def generate_complete_briefing(context: Any, structure: Dict[str, Any], cache_status: Dict[str, Any] = None) -> str:
    """ì™„ì „í•œ í”„ë¡œì íŠ¸ ë¸Œë¦¬í•‘ ìƒì„± - ProjectContext ì „ìš©"""
    briefing_lines = []
    cache_status = cache_status or {}
    debug_log(f"ğŸ› ë¸Œë¦¬í•‘ í•¨ìˆ˜: cache_status = {cache_status}")
    
    # context íƒ€ì… í™•ì¸
    from core.models import ProjectContext
    is_pydantic = isinstance(context, ProjectContext)
    
    # Pydanticì´ ì•„ë‹ˆë©´ ë³€í™˜ ì‹œë„
    if not is_pydantic:
        try:
            context = ProjectContext.from_dict(context)
            is_pydantic = True
        except:
            # ë³€í™˜ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ dictë¡œ ì²˜ë¦¬
            pass
    
    # ì¦‰ì‹œ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ í‘œì‹œ
    briefing_lines.append("\n" + "="*70)
    briefing_lines.append("ğŸ“Š **í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ (ì¦‰ì‹œ í‘œì‹œ)**")
    briefing_lines.append("="*70)
    
    if is_pydantic:
        briefing_lines.append(f"ğŸ¯ í”„ë¡œì íŠ¸: {context.project_name}")
        briefing_lines.append(f"ğŸ“ Phase: {getattr(context, 'metadata', {}).get('phase', 'initialization')}")
        
        current_task = context.current_task
        if current_task and isinstance(current_task, dict):
            briefing_lines.append(f"âœï¸ í˜„ì¬ ì‘ì—…: [{current_task.get('id', 'N/A')}] {current_task.get('title', 'ì—†ìŒ')}")
        elif current_task:
            briefing_lines.append(f"âœï¸ í˜„ì¬ ì‘ì—…: {current_task}")
        else:
            briefing_lines.append(f"âœï¸ í˜„ì¬ ì‘ì—…: ì„¤ì •ë˜ì§€ ì•ŠìŒ")
    else:
        # dict ì²˜ë¦¬
        briefing_lines.append(f"ğŸ¯ í”„ë¡œì íŠ¸: {context.get('project_name', 'Unknown')}")
        briefing_lines.append(f"ğŸ“ Phase: {context.get('phase', 'initialization')}")
        
        current_task = context.get('current_task')
        if current_task and isinstance(current_task, dict):
            briefing_lines.append(f"âœï¸ í˜„ì¬ ì‘ì—…: [{current_task.get('id', 'N/A')}] {current_task.get('title', 'ì—†ìŒ')}")
        else:
            briefing_lines.append(f"âœï¸ í˜„ì¬ ì‘ì—…: ì„¤ì •ë˜ì§€ ì•ŠìŒ")
    
    # tasks ì²˜ë¦¬
    if is_pydantic and context.plan:
        all_tasks = context.get_all_tasks()
        pending_tasks = [t for t in all_tasks if not t.completed]
        completed_tasks = [t for t in all_tasks if t.completed]
        
        if all_tasks:
            briefing_lines.append(f"\n  â€¢ ì „ì²´ ì‘ì—…: {len(all_tasks)}ê°œ (ì™„ë£Œ: {len(completed_tasks)}, ëŒ€ê¸°: {len(pending_tasks)})")
        else:
            briefing_lines.append(f"\n  â€¢ ì‘ì—… í˜„í™©: ë“±ë¡ëœ ì‘ì—… ì—†ìŒ")
    else:
        briefing_lines.append(f"ğŸ“‹ ì‘ì—… í˜„í™©: ë“±ë¡ëœ ì‘ì—… ì—†ìŒ")
    
    briefing_lines.append("\n" + "="*70)
    briefing_lines.append("ğŸ“Š **í”„ë¡œì íŠ¸ ìƒíƒœ ë¸Œë¦¬í•‘**")
    briefing_lines.append("="*70)
    
    # 1. í”„ë¡œì íŠ¸ ì •ë³´
    briefing_lines.append(f"\nğŸ“ **í”„ë¡œì íŠ¸ ì •ë³´**")
    if is_pydantic:
        briefing_lines.append(f"  â€¢ ì´ë¦„: {context.project_name}")
        briefing_lines.append(f"  â€¢ ê²½ë¡œ: {context.project_path}")
        briefing_lines.append(f"  â€¢ ì–¸ì–´: Python/TypeScript")
    else:
        briefing_lines.append(f"  â€¢ ì´ë¦„: {context.get('project_name', 'Unknown')}")
        briefing_lines.append(f"  â€¢ ê²½ë¡œ: {context.get('project_path', os.getcwd())}")
        briefing_lines.append(f"  â€¢ ì–¸ì–´: {context.get('language', 'Python/TypeScript')}")
    
    # íŒŒì¼ í†µê³„
    stats = structure.get("statistics", {})
    briefing_lines.append(f"  â€¢ ì „ì²´ íŒŒì¼: {stats.get('total_files', 0)}ê°œ")
    
    # ì£¼ìš” í™•ì¥ìë³„ í†µê³„
    ext_stats = stats.get("by_extension", {})
    if ext_stats:
        top_exts = sorted(ext_stats.items(), key=lambda x: x[1], reverse=True)[:5]
        ext_str = ", ".join([f"{ext or 'no-ext'}: {count}" for ext, count in top_exts])
        briefing_lines.append(f"  â€¢ íŒŒì¼ íƒ€ì…: {ext_str}")
    
    # 2. í”„ë¡œì íŠ¸ êµ¬ì¡° í•˜ì´ë¼ì´íŠ¸
    briefing_lines.append(f"\nğŸ“‚ **í”„ë¡œì íŠ¸ êµ¬ì¡°**")
    key_files = structure.get("key_files", {})
    
    if key_files.get("entry_points"):
        briefing_lines.append(f"  â€¢ ì§„ì…ì : {', '.join(key_files['entry_points'][:3])}")
    
    if key_files.get("configs"):
        briefing_lines.append(f"  â€¢ ì„¤ì • íŒŒì¼: {', '.join(key_files['configs'][:3])}")
    
    # 3. ìµœê·¼ ìˆ˜ì •ëœ íŒŒì¼ë“¤
    briefing_lines.append(f"\nğŸ“ **ìµœê·¼ ìˆ˜ì •ëœ íŒŒì¼** (ìƒìœ„ 5ê°œ)")
    recent_files = structure.get("recent_files", [])[:5]
    for file_info in recent_files:
        modified = file_info["modified"].split("T")[0]  # ë‚ ì§œë§Œ
        briefing_lines.append(f"  â€¢ {file_info['path']} ({modified})")
    
    # 4. í˜„ì¬ ì‘ì—… - ë” ìì„¸íˆ
    briefing_lines.append(f"\nğŸ“‹ **ì‘ì—… ìƒíƒœ**")
    if is_pydantic:
        if context.current_task and context.plan:
            task_obj = context.plan.get_task_by_id(context.current_task)
            if task_obj:
                briefing_lines.append(f"  â€¢ í˜„ì¬ ì‘ì—…: [{task_obj.id}] {task_obj.title}")
                briefing_lines.append(f"  â€¢ ìƒíƒœ: {'âœ… ì™„ë£Œ' if task_obj.completed else 'â³ ì§„í–‰ ì¤‘'}")
            else:
                briefing_lines.append(f"  â€¢ í˜„ì¬ ì‘ì—…: {context.current_task}")
        else:
            briefing_lines.append(f"  â€¢ í˜„ì¬ ì‘ì—…: ì—†ìŒ")
        
        # ëŒ€ê¸° ì¤‘ì¸ ì‘ì—…ë“¤ (ìƒìœ„ 5ê°œ)
        if context.plan:
            all_tasks = context.get_all_tasks()
            pending_tasks = [t for t in all_tasks if not t.completed][:5]
            if pending_tasks:
                briefing_lines.append(f"\n  ğŸ“Œ ëŒ€ê¸° ì¤‘ì¸ ì‘ì—…:")
                for task in pending_tasks:
                    briefing_lines.append(f"    - [{task.id}] {task.title}")
    else:
        current_task = context.get('current_task') if not is_pydantic else None
        if current_task:
            briefing_lines.append(f"  â€¢ í˜„ì¬ ì‘ì—…: [{current_task.get('id', 'N/A')}] {current_task.get('title', 'ì œëª© ì—†ìŒ')}")
            briefing_lines.append(f"  â€¢ ìƒíƒœ: {'âœ… ì™„ë£Œ' if current_task.get('completed') else 'â³ ì§„í–‰ ì¤‘'}")
        else:
            briefing_lines.append(f"  â€¢ í˜„ì¬ ì‘ì—…: ì—†ìŒ")
    

    # 4-1. ì§„í–‰ë¥  í‘œì‹œ
    if is_pydantic and context.plan:
        all_tasks = context.get_all_tasks()
        completed_tasks = [t for t in all_tasks if t.completed]
        pending_tasks = [t for t in all_tasks if not t.completed]
        in_progress_tasks = [t for t in pending_tasks if hasattr(t, 'in_progress') and t.in_progress]
        
        if all_tasks:
            total_progress = (len(completed_tasks) / len(all_tasks) * 100)
            briefing_lines.append(f"\nğŸ“Š **ì „ì²´ ì§„í–‰ë¥ **: {total_progress:.1f}%")
            briefing_lines.append(f"  â€¢ âœ… ì™„ë£Œ: {len(completed_tasks)}ê°œ")
            briefing_lines.append(f"  â€¢ ğŸ”„ ì§„í–‰ ì¤‘: {len(in_progress_tasks)}ê°œ")
            briefing_lines.append(f"  â€¢ â³ ëŒ€ê¸°: {len(pending_tasks) - len(in_progress_tasks)}ê°œ")
            
            # ë‹¤ìŒ í•´ì•¼ í•  ì¼
            if pending_tasks:
                briefing_lines.append(f"\nğŸ¯ **ë‹¤ìŒ í•´ì•¼ í•  ì¼**")
                next_task = pending_tasks[0]
                briefing_lines.append(f"  â€¢ ë‹¤ìŒ ì‘ì—…: [{next_task.id}] {next_task.title}")
                if hasattr(next_task, 'estimated_time'):
                    briefing_lines.append(f"  â€¢ ì˜ˆìƒ ì†Œìš”ì‹œê°„: {next_task.estimated_time}")
                if hasattr(next_task, 'description') and next_task.description:
                    desc_preview = next_task.description[:100] + "..." if len(next_task.description) > 100 else next_task.description
                    briefing_lines.append(f"  â€¢ ì‘ì—… ì„¤ëª…: {desc_preview}")
                briefing_lines.append(f"\n  ğŸ’¡ ì‹œì‘í•˜ë ¤ë©´: next_task() ëª…ë ¹ì„ ì‚¬ìš©í•˜ì„¸ìš”")

    # 5. Phase ì •ë³´ ì¶”ê°€
    phase = getattr(context, 'phase', None) if is_pydantic else context.get('phase')
    if phase:
        briefing_lines.append(f"\nğŸ¯ **í˜„ì¬ Phase**: {phase}")
        phase_goals = {
            "initialization": "í”„ë¡œì íŠ¸ êµ¬ì¡° íŒŒì•… ë° ì´ˆê¸° ì„¤ì •",
            "planning": "ì‘ì—… ê³„íš ìˆ˜ë¦½ ë° ìš°ì„ ìˆœìœ„ ê²°ì •", 
            "development": "ê¸°ëŠ¥ ê°œë°œ ë° ì½”ë“œ ì‘ì„±",
            "testing": "í…ŒìŠ¤íŠ¸ ì‘ì„± ë° ì‹¤í–‰",
            "refactoring": "ì½”ë“œ ê°œì„  ë° ìµœì í™”",
            "deployment": "ë°°í¬ ì¤€ë¹„ ë° ì‹¤í–‰",
            "maintenance": "ë²„ê·¸ ìˆ˜ì • ë° ìœ ì§€ë³´ìˆ˜"
        }
        briefing_lines.append(f"  â€¢ Phase ëª©í‘œ: {phase_goals.get(phase, 'ì„¤ì •ë˜ì§€ ì•ŠìŒ')}")

    
    # 6. Wisdom í†µê³„ ì¶”ê°€
def find_project_root(project_name: str) -> Optional[Path]:
    """í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°"""
    # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ê°€ í”„ë¡œì íŠ¸ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    current_dir = Path.cwd()
    if current_dir.name == project_name:
        return current_dir
    
    # ìƒìœ„ ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
    for parent in current_dir.parents:
        if parent.name == project_name:
            return parent
    
    # Desktopì—ì„œ ì°¾ê¸°
    desktop_path = Path.home() / "Desktop" / project_name
    if desktop_path.exists():
        return desktop_path
    
    # í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ í•˜ìœ„ì—ì„œ ì°¾ê¸°
    for child in current_dir.iterdir():
        if child.is_dir() and child.name == project_name:
            return child
    
    return None


def display_project_briefing(context, analyzer_result: Dict, wisdom_insights: Dict, verbose: bool = True):
    """í”„ë¡œì íŠ¸ ë¸Œë¦¬í•‘ í‘œì‹œ (ê°„ê²°/ìƒì„¸ ëª¨ë“œ ì§€ì›)"""
    print("\n" + "=" * 70)
    print("ğŸ“Š **í”„ë¡œì íŠ¸ ìƒíƒœ ë¸Œë¦¬í•‘**")
    print("=" * 70)
    
    # ê¸°ë³¸ ì •ë³´ (í•­ìƒ í‘œì‹œ)
    print(f"\nğŸ“ **í”„ë¡œì íŠ¸ ì •ë³´**")
    print(f"  â€¢ ì´ë¦„: {context.project_name}")
    print(f"  â€¢ ê²½ë¡œ: {context.project_path}")
    print(f"  â€¢ ì–¸ì–´: {context.language or 'Unknown'}")
    print(f"  â€¢ ì „ì²´ íŒŒì¼: {len(analyzer_result.get('all_files', []))}ê°œ")
    
    if verbose:
        # ìƒì„¸ ì •ë³´
        print(f"\nğŸ“‚ **í”„ë¡œì íŠ¸ êµ¬ì¡°**")
        dirs = analyzer_result.get('structure', {}).get('directories', [])
        print(f"  â€¢ ë””ë ‰í† ë¦¬: {len(dirs)}ê°œ")
        
        # ìµœê·¼ ìˆ˜ì • íŒŒì¼
        if analyzer_result.get('modified_files'):
            print(f"\nğŸ”„ **ìµœê·¼ ìˆ˜ì •ëœ íŒŒì¼** (ìƒìœ„ 5ê°œ):")
            for file in analyzer_result['modified_files'][:5]:
                print(f"  â€¢ {file['path']}")
    
    # Wisdom ì¸ì‚¬ì´íŠ¸ (ì¤‘ìš”í•œ ê²ƒë§Œ)
    if wisdom_insights.get('warnings'):
        print(f"\nâš ï¸ **ì£¼ì˜ì‚¬í•­**")
        for warning in wisdom_insights['warnings']:
            print(f"  {warning}")
    
    if wisdom_insights.get('recent_mistakes') and verbose:
        print(f"\nâŒ **ìµœê·¼ ì‹¤ìˆ˜** (ì£¼ì˜í•˜ì„¸ìš”!):")
        for mistake in wisdom_insights['recent_mistakes'][:3]:
            print(f"  â€¢ {mistake['type']}: {mistake['count']}íšŒ")
    
    print("\n" + "=" * 70)


def track_flow_performance(func_name: str, duration: float, success: bool):
    """flow ëª…ë ¹ ì„±ëŠ¥ ì¶”ì """
    if not PERFORMANCE_TRACKING:
        return
    
    perf_data = wisdom.wisdom_data.get('performance_metrics', {})
    
    if func_name not in perf_data:
        perf_data[func_name] = {
            'total_calls': 0,
            'successful_calls': 0,
            'total_duration': 0,
            'average_duration': 0
        }
    
    perf_data[func_name]['total_calls'] += 1
    if success:
        perf_data[func_name]['successful_calls'] += 1
    perf_data[func_name]['total_duration'] += duration
    perf_data[func_name]['average_duration'] = (
        perf_data[func_name]['total_duration'] / perf_data[func_name]['total_calls']
    )
    
    wisdom.wisdom_data['performance_metrics'] = perf_data
    wisdom._save_wisdom()


def validate_cache_integrity(cache_path: str) -> bool:
    """ìºì‹œ íŒŒì¼ ë¬´ê²°ì„± ê²€ì¦"""
    try:
        if not os.path.exists(cache_path):
            return False
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        size_mb = os.path.getsize(cache_path) / (1024 * 1024)
        if size_mb > MAX_CACHE_SIZE_MB:
            smart_print(f"âš ï¸ ìºì‹œ í¬ê¸° ì´ˆê³¼: {size_mb:.1f}MB > {MAX_CACHE_SIZE_MB}MB")
            return False
        
        # ìºì‹œ ë§Œë£Œ í™•ì¸
        mod_time = datetime.fromtimestamp(os.path.getmtime(cache_path))
        age_hours = (datetime.now() - mod_time).total_seconds() / 3600
        if age_hours > CACHE_EXPIRY_HOURS:
            smart_print(f"âš ï¸ ìºì‹œ ë§Œë£Œ: {age_hours:.1f}ì‹œê°„ ê²½ê³¼")
            return False
        
        return True
    except Exception as e:
        smart_print(f"âŒ ìºì‹œ ê²€ì¦ ì‹¤íŒ¨: {e}")
        return False


def create_context_backup(project_name: str, context: Any) -> Optional[str]:
    """í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ìë™ ë°±ì—…"""
    try:
        # ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
        backup_dir = Path("memory/context_backups") / datetime.now().strftime("%Y-%m-%d")
        backup_dir.mkdir(parents=True, exist_ok=True)
        
        # ë°±ì—… íŒŒì¼ëª…
        timestamp = datetime.now().strftime("%H%M%S")
        backup_file = backup_dir / f"{project_name}_context_{timestamp}.json"
        
        # ì»¨í…ìŠ¤íŠ¸ë¥¼ JSONìœ¼ë¡œ ì €ì¥
        context_data = {
            'project_name': project_name,
            'backup_time': datetime.now().isoformat(),
            'context': context if isinstance(context, dict) else context.dict() if hasattr(context, 'dict') else str(context)
        }
        
        # datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” default í•¨ìˆ˜
        def json_default(obj):
            if isinstance(obj, datetime):
                return obj.isoformat()
            elif hasattr(obj, '__dict__'):
                return obj.__dict__
            return str(obj)
        
        with open(backup_file, 'w', encoding='utf-8') as f:
            json.dump(context_data, f, indent=2, ensure_ascii=False, default=json_default)
        
        # ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬ (7ì¼ ì´ìƒ)
        cleanup_old_backups(backup_dir.parent, days=7)
        
        return str(backup_file)
    except Exception as e:
        smart_print(f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ ë°±ì—… ì‹¤íŒ¨: {e}")
        return None


def cleanup_old_backups(backup_root: Path, days: int = 7):
    """ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ ì •ë¦¬"""
    cutoff_date = datetime.now() - timedelta(days=days)
    
    for date_dir in backup_root.iterdir():
        if date_dir.is_dir():
            try:
                dir_date = datetime.strptime(date_dir.name, "%Y-%m-%d")
                if dir_date < cutoff_date:
                    shutil.rmtree(date_dir)
                    smart_print(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ ë°±ì—… ì‚­ì œ: {date_dir.name}")
            except:
                pass


def load_project_config(project_path: Path) -> Dict[str, Any]:
    """í”„ë¡œì íŠ¸ë³„ ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    config_file = project_path / ".ai-brain.config.json"
    default_config = {
        "verbose": True,
        "auto_backup": True,
        "scan_exclude": ["node_modules", ".git", "__pycache__", "dist", "build"],
        "file_limit": 1000,
        "cache_expiry_hours": 24
    }
    
    if config_file.exists():
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
                # ê¸°ë³¸ ì„¤ì •ê³¼ ë³‘í•©
                default_config.update(user_config)
                smart_print(f"ğŸ“‹ í”„ë¡œì íŠ¸ ì„¤ì • ë¡œë“œ: {config_file.name}")
        except Exception as e:
            smart_print(f"âš ï¸ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return default_config


def save_project_config(project_path: Path, config: Dict[str, Any]):
    """í”„ë¡œì íŠ¸ ì„¤ì • ì €ì¥"""
    config_file = project_path / ".ai-brain.config.json"
    try:
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        smart_print(f"ğŸ’¾ í”„ë¡œì íŠ¸ ì„¤ì • ì €ì¥: {config_file.name}")
    except Exception as e:
        smart_print(f"âŒ ì„¤ì • ì €ì¥ ì‹¤íŒ¨: {e}")


def analyze_folder_with_context(folder_path: str, recursive: bool = True, extensions: list = None) -> Dict[str, Any]:
    """
    í´ë”ì™€ í•˜ìœ„ íŒŒì¼ë“¤ì„ ë¶„ì„í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ìƒì„±
    
    Args:
        folder_path: ë¶„ì„í•  í´ë” ê²½ë¡œ
        recursive: í•˜ìœ„ í´ë”ë„ ë¶„ì„í• ì§€ ì—¬ë¶€
        extensions: ë¶„ì„í•  íŒŒì¼ í™•ì¥ì ëª©ë¡
        
    Returns:
        í´ë” ë¶„ì„ ê²°ê³¼ (êµ¬ì¡°, íŒŒì¼ ì •ë³´, í†µê³„ ë“±)
    """
    from pathlib import Path
    
    folder_path = Path(folder_path)
    if not folder_path.exists():
        return {"error": f"í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {folder_path}"}
    
    if extensions is None:
        extensions = ['.py', '.ts', '.js', '.jsx', '.tsx', '.md', '.json']
    
    result = {
        "folder": str(folder_path),
        "name": folder_path.name,
        "structure": {},
        "files": {},
        "statistics": {
            "total_files": 0,
            "total_dirs": 0,
            "by_extension": {},
            "total_lines": 0,
            "total_functions": 0,
            "total_classes": 0
        }
    }
    
    # í´ë” êµ¬ì¡° êµ¬ì¶•
    def build_structure(path: Path, parent_dict: dict):
        try:
            for item in path.iterdir():
                if item.is_dir():
                    if item.name not in ['.git', '__pycache__', 'node_modules', '.venv', 'dist', 'build']:
                        result["statistics"]["total_dirs"] += 1
                        parent_dict[item.name] = {}
                        if recursive:
                            build_structure(item, parent_dict[item.name])
                elif item.is_file() and item.suffix in extensions:
                    parent_dict[item.name] = "file"
                    result["statistics"]["total_files"] += 1
                    
                    # íŒŒì¼ í†µê³„
                    ext = item.suffix
                    result["statistics"]["by_extension"][ext] = result["statistics"]["by_extension"].get(ext, 0) + 1
                    
                    # íŒŒì¼ ë¶„ì„ (Python íŒŒì¼ì˜ ê²½ìš°)
                    if item.suffix == '.py':
                        try:
                            relative_path = item.relative_to(folder_path)
                            file_result = helpers.parse_with_snippets(str(item))
                            if file_result['parsing_success']:
                                result["files"][str(relative_path)] = {
                                    "functions": len(file_result.get('functions', [])),
                                    "classes": len(file_result.get('classes', [])),
                                    "lines": file_result.get('line_count', 0)
                                }
                                result["statistics"]["total_functions"] += len(file_result.get('functions', []))
                                result["statistics"]["total_classes"] += len(file_result.get('classes', []))
                                result["statistics"]["total_lines"] += file_result.get('line_count', 0)
                        except:
                            pass
        except PermissionError:
            pass
    
    build_structure(folder_path, result["structure"])
    
    # ìš”ì•½ ì •ë³´ ì¶”ê°€
    result["summary"] = generate_folder_summary(result)
    
    return result


def generate_folder_summary(analysis: Dict[str, Any]) -> str:
    """í´ë” ë¶„ì„ ê²°ê³¼ë¥¼ ìš”ì•½"""
    stats = analysis['statistics']
    summary = f"""ğŸ“‚ {analysis['name']}
â”œâ”€ ğŸ“„ íŒŒì¼: {stats['total_files']}ê°œ
â”œâ”€ ğŸ“ í´ë”: {stats['total_dirs']}ê°œ
â”œâ”€ ğŸ“ ì´ ë¼ì¸: {stats['total_lines']:,}ì¤„
â”œâ”€ ğŸ”§ í•¨ìˆ˜: {stats['total_functions']}ê°œ
â”œâ”€ ğŸ—ï¸ í´ë˜ìŠ¤: {stats['total_classes']}ê°œ
â””â”€ ğŸ“Š íŒŒì¼ ìœ í˜•: {', '.join(f"{k}({v})" for k, v in stats['by_extension'].items())}"""
    return summary


def flow_analyze_folder(folder_path: str, save_context: bool = True, verbose: bool = True) -> Dict[str, Any]:
    """
    íŠ¹ì • í´ë”ë¥¼ ë¶„ì„í•˜ê³  í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
    
    Args:
        folder_path: ë¶„ì„í•  í´ë” ê²½ë¡œ
        save_context: ì»¨í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í• ì§€ ì—¬ë¶€
        verbose: ìƒì„¸ ì •ë³´ ì¶œë ¥ ì—¬ë¶€
    """
    smart_print(f"\nğŸ“‚ í´ë” ë¶„ì„ ì‹œì‘: {folder_path}")
    
    # 1. í´ë” ë¶„ì„
    analysis = analyze_folder_with_context(folder_path, recursive=True)
    
    if "error" in analysis:
        smart_print(f"âŒ {analysis['error']}")
        return analysis
    
    # 2. ë¶„ì„ ê²°ê³¼ ì¶œë ¥
    if verbose:
        smart_print("\n" + analysis['summary'])
        
        # ì£¼ìš” íŒŒì¼ ì •ë³´
        if analysis['files']:
            smart_print("\nğŸ“„ ì£¼ìš” íŒŒì¼:")
            for file_path, info in list(analysis['files'].items())[:5]:
                smart_print(f"  - {file_path}: {info.get('functions', 0)}ê°œ í•¨ìˆ˜, {info.get('lines', 0)}ì¤„")
    
    # 3. ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    folder_context = {
        "type": "folder_analysis",
        "path": folder_path,
        "name": analysis['name'],
        "analysis": analysis,
        "timestamp": datetime.now().isoformat()
    }
    
    # 4. ì»¨í…ìŠ¤íŠ¸ ì €ì¥
    if save_context:
        # í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
        try:
            current_context = helpers.get_context()
            if hasattr(current_context, 'metadata'):
                if 'folder_analyses' not in current_context.metadata:
                    current_context.metadata['folder_analyses'] = {}
                current_context.metadata['folder_analyses'][analysis['name']] = folder_context
                helpers.save_context()
                smart_print(f"\nğŸ’¾ í´ë” ë¶„ì„ ê²°ê³¼ê°€ ì»¨í…ìŠ¤íŠ¸ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except:
            pass
        
        # ë³„ë„ íŒŒì¼ë¡œë„ ì €ì¥
        context_file = Path(f"memory/folder_contexts/{analysis['name']}_context.json")
        context_file.parent.mkdir(parents=True, exist_ok=True)
        with open(context_file, 'w', encoding='utf-8') as f:
            json.dump(folder_context, f, indent=2, ensure_ascii=False)
        smart_print(f"ğŸ“ ì»¨í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥: {context_file.name}")
    
    # 5. Wisdom ì‹œìŠ¤í…œì— ê¸°ë¡ (ì œê±°ë¨)
    
    return folder_context


def flow_compare_folders(folder1: str, folder2: str) -> Dict[str, Any]:
    """ë‘ í´ë”ë¥¼ ë¹„êµ ë¶„ì„"""
    smart_print(f"\nğŸ”„ í´ë” ë¹„êµ: {folder1} vs {folder2}")
    
    # ê° í´ë” ë¶„ì„
    analysis1 = analyze_folder_with_context(folder1)
    analysis2 = analyze_folder_with_context(folder2)
    
    # ë¹„êµ ê²°ê³¼
    comparison = {
        "folder1": folder1,
        "folder2": folder2,
        "differences": {
            "files": {
                "only_in_1": [],
                "only_in_2": [],
                "in_both": []
            },
            "statistics": {
                "total_files": {
                    folder1: analysis1['statistics']['total_files'],
                    folder2: analysis2['statistics']['total_files']
                },
                "total_lines": {
                    folder1: analysis1['statistics']['total_lines'],
                    folder2: analysis2['statistics']['total_lines']
                }
            }
        }
    }
    
    # íŒŒì¼ ë¹„êµ
    files1 = set(analysis1['files'].keys())
    files2 = set(analysis2['files'].keys())
    
    comparison['differences']['files']['only_in_1'] = list(files1 - files2)
    comparison['differences']['files']['only_in_2'] = list(files2 - files1)
    comparison['differences']['files']['in_both'] = list(files1 & files2)
    
    # ê²°ê³¼ ì¶œë ¥
    smart_print(f"\nğŸ“Š ë¹„êµ ê²°ê³¼:")
    smart_print(f"  - {folder1}ì—ë§Œ ìˆëŠ” íŒŒì¼: {len(comparison['differences']['files']['only_in_1'])}ê°œ")
    smart_print(f"  - {folder2}ì—ë§Œ ìˆëŠ” íŒŒì¼: {len(comparison['differences']['files']['only_in_2'])}ê°œ")
    smart_print(f"  - ê³µí†µ íŒŒì¼: {len(comparison['differences']['files']['in_both'])}ê°œ")
    
    return comparison


def flow_project(project_name: str, verbose: Optional[bool] = None) -> Dict[str, Any]:
    """ë¦¬íŒ©í† ë§ëœ flow_project - ìë™ ë°±ì—… ë° í”„ë¡œì íŠ¸ ì„¤ì • ì§€ì›"""
    import time
    start_time = time.time()
    
    smart_print(f"ğŸš€ **'{project_name}'** í”„ë¡œì íŠ¸ ì„¸ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    # helpersë¥¼ ì „ì—­ì—ì„œ ê°€ì ¸ì˜¤ê¸°
    helpers_obj = None
    
    # 1. global_helpers í™•ì¸ (json_repl_sessionì—ì„œ ì„¤ì •)
    if 'global_helpers' in globals():
        helpers_obj = globals()['global_helpers']
    # 2. ì§ì ‘ ì „ì—­ì—ì„œ ì°¾ê¸°
    elif 'helpers' in globals():
        helpers_obj = globals()['helpers']
    # 3. __main__ ëª¨ë“ˆì—ì„œ ì°¾ê¸°
    elif hasattr(sys.modules.get('__main__', None), 'helpers'):
        helpers_obj = sys.modules.get('__main__').helpers
    
    if not helpers_obj:
        raise RuntimeError("helpers ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. execute_code í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”.")
    
    result = {
        'success': False,
        'project_name': project_name,
        'project_path': None,
        'context': None,
        'analysis': None,
        'error': None,
        'backup_path': None
    }
    
    # ì´ì „ ì»¨í…ìŠ¤íŠ¸ ë°±ì—…
    try:
        current_context = helpers_obj.get_context()
        if current_context and hasattr(current_context, 'project_name'):
            if current_context.project_name != project_name:
                backup_path = create_context_backup(current_context.project_name, current_context)
                if backup_path:
                    smart_print(f"ğŸ’¾ ì´ì „ í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ ë°±ì—…: {Path(backup_path).name}")
                    result['backup_path'] = backup_path
    except:
        pass  # ë°±ì—… ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
    
    try:
        # 1. í”„ë¡œì íŠ¸ ì´ë¦„ ì¶”ì¶œ ë° ê²½ë¡œ ì°¾ê¸°
        clean_name = project_name.strip()  # extract_project_name ëŒ€ì‹  ì§ì ‘ ì²˜ë¦¬
        project_path = find_project_root(clean_name)  # find_project_path ëŒ€ì‹  find_project_root ì‚¬ìš©
        
        if not project_path:
            raise ValueError(f"í”„ë¡œì íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {clean_name}")
        
        result['project_path'] = str(project_path)
        
        # 2. ì‘ì—… ë””ë ‰í† ë¦¬ ë³€ê²½
        os.chdir(project_path)
        smart_print(f"ğŸ“ ì‘ì—… ë””ë ‰í† ë¦¬ ë³€ê²½: {project_path}")
        
        # 3. í”„ë¡œì íŠ¸ ì„¤ì • ë¡œë“œ
        project_config = load_project_config(project_path)
        
        # verbose ë§¤ê°œë³€ìˆ˜ê°€ Noneì´ë©´ ì„¤ì • íŒŒì¼ì˜ ê°’ ì‚¬ìš©
        if verbose is None:
            verbose = project_config.get('verbose', True)
        
        # 4. Context ì´ˆê¸°í™” ë° ë¡œë“œ
        helpers_obj.initialize_context(project_path)
        context = helpers_obj.get_context()
        
        # ì»¨í…ìŠ¤íŠ¸ê°€ Noneì´ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì´ˆê¸°í™”
        if context is None:
            context = {
                'project_name': clean_name,
                'project_path': str(project_path),
                'created_at': datetime.now().isoformat()
            }
        
        result['context'] = context
        
        # 5. í”„ë¡œì íŠ¸ ìë™ ë¶„ì„ ë˜ëŠ” ìºì‹œ ì‚¬ìš©
        smart_print("\nğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ëŠ” ì¤‘...")
        
        analyzer = ProjectAnalyzer(project_path)
        # ì„¤ì •ì—ì„œ ì œì™¸ íŒ¨í„´ ì ìš©
        if hasattr(analyzer, 'ignore_patterns'):
            analyzer.ignore_patterns.extend(project_config.get('scan_exclude', []))
        
        analysis_result = analyzer.analyze_and_update()
        result['analysis'] = analysis_result
        
        # 6. Wisdom ì¸ì‚¬ì´íŠ¸ ê°€ì ¸ì˜¤ê¸° (ì œê±°ë¨)
        
        # ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë¨
        result['success'] = True
        
        return result
    
    except Exception as e:
        smart_print(f"âŒ í”„ë¡œì íŠ¸ ì „í™˜ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {
            'success': False,
            'error': str(e),
            'project_name': project_name
        }


def flow_project_legacy(project_name: str) -> Dict[str, Any]:
    """[DEPRECATED] flow_project()ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”"""
    print("âš ï¸ flow_project_legacyëŠ” deprecatedë˜ì—ˆìŠµë‹ˆë‹¤.")
    return flow_project(project_name, verbose=True)