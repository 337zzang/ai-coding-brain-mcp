"""
ğŸ” í†µí•© ê²€ìƒ‰ ì‹œìŠ¤í…œ (Integrated Search System)
============================================

ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ë¥¼ í•œë²ˆì— ê²€ìƒ‰í•˜ëŠ” í†µí•© ê²€ìƒ‰ ì—”ì§„
- Context Manager (ë©”ëª¨ë¦¬ ìºì‹œ)
- Vibe Memory (ë¡œì»¬ .md íŒŒì¼)
- í”„ë¡œì íŠ¸ ìºì‹œ (.json íŒŒì¼)
- Claude Memory (ê°œë°œ ê²½í—˜)

ì‘ì„±ì¼: 2025-06-10
"""

import os
import json
import re
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Union
from pathlib import Path


class IntegratedSearchEngine:
    """í†µí•© ê²€ìƒ‰ ì—”ì§„"""
    
    def __init__(self, project_context: dict):
        self.project_context = project_context
        self.project_path = project_context.get('storage', {}).get('base_path', '.')
        self.memory_bank_path = project_context.get('memory_bank', {}).get('project_path', '')
        
        # ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ (ì„±ëŠ¥ ìµœì í™”)
        self._search_cache = {}
        
    def search(self, query: str, search_type: str = 'all', limit: int = 20) -> Dict[str, Any]:
        """
        í†µí•© ê²€ìƒ‰ ì‹¤í–‰
        
        Args:
            query: ê²€ìƒ‰ì–´
            search_type: 'all', 'code', 'tasks', 'memory', 'cache'
            limit: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            
        Returns:
            í†µí•©ëœ ê²€ìƒ‰ ê²°ê³¼
        """
        print(f"\nğŸ” í†µí•© ê²€ìƒ‰: '{query}' (íƒ€ì…: {search_type})")
        
        results = {
            'query': query,
            'timestamp': datetime.now().isoformat(),
            'sources': {},
            'total_results': 0
        }
        
        # 1. Context Manager ê²€ìƒ‰
        if search_type in ['all', 'code', 'cache']:
            context_results = self._search_context_manager(query)
            if context_results:
                results['sources']['context_manager'] = context_results
                results['total_results'] += len(context_results)
        
        # 2. Vibe Memory ê²€ìƒ‰
        if search_type in ['all', 'tasks', 'memory']:
            vibe_results = self._search_vibe_memory(query)
            if vibe_results:
                results['sources']['vibe_memory'] = vibe_results
                results['total_results'] += sum(len(v) for v in vibe_results.values())
        
        # 3. í”„ë¡œì íŠ¸ ìºì‹œ ê²€ìƒ‰
        if search_type in ['all', 'cache']:
            cache_results = self._search_project_cache(query)
            if cache_results:
                results['sources']['project_cache'] = cache_results
                results['total_results'] += len(cache_results)
        
        # 4. Claude Memory ê²€ìƒ‰
        if search_type in ['all', 'memory']:
            memory_results = self._search_claude_memory(query)
            if memory_results:
                results['sources']['claude_memory'] = memory_results
                results['total_results'] += len(memory_results)
        
        # ê²°ê³¼ ì •ë ¬ ë° ì œí•œ
        results = self._rank_and_limit_results(results, limit)
        
        return results
    
    def _search_context_manager(self, query: str) -> List[Dict[str, Any]]:
        """Context Manager ìºì‹œì—ì„œ ê²€ìƒ‰"""
        results = []
        query_lower = query.lower()
        
        cache = self.project_context.get('cache', {})
        
        # 1. symbol_index ê²€ìƒ‰
        symbol_index = cache.get('symbol_index', {})
        for symbol, file_path in symbol_index.items():
            if query_lower in symbol.lower():
                results.append({
                    'type': 'symbol',
                    'name': symbol,
                    'file': file_path,
                    'source': 'symbol_index',
                    'relevance': self._calculate_relevance(query_lower, symbol.lower())
                })
        
        # 2. analyzed_files ê²€ìƒ‰
        analyzed_files = cache.get('analyzed_files', {})
        for file_path, file_data in analyzed_files.items():
            if isinstance(file_data, dict) and 'data' in file_data:
                analysis = file_data['data']
                
                # í•¨ìˆ˜ ê²€ìƒ‰
                for func in analysis.get('functions', []):
                    if query_lower in func.get('name', '').lower():
                        results.append({
                            'type': 'function',
                            'name': func['name'],
                            'file': file_path,
                            'line': func.get('start_line'),
                            'source': 'analyzed_files',
                            'relevance': self._calculate_relevance(query_lower, func['name'].lower())
                        })
                
                # í´ë˜ìŠ¤ ê²€ìƒ‰
                for cls in analysis.get('classes', []):
                    if query_lower in cls.get('name', '').lower():
                        results.append({
                            'type': 'class',
                            'name': cls['name'],
                            'file': file_path,
                            'line': cls.get('start_line'),
                            'source': 'analyzed_files',
                            'relevance': self._calculate_relevance(query_lower, cls['name'].lower())
                        })
        
        # 3. recent_operations ê²€ìƒ‰
        recent_ops = cache.get('recent_operations', [])
        for op in recent_ops:
            if query_lower in str(op).lower():
                results.append({
                    'type': 'operation',
                    'operation': op.get('operation'),
                    'target': op.get('target'),
                    'timestamp': op.get('timestamp'),
                    'source': 'recent_operations',
                    'relevance': 0.5
                })
        
        return results
    
    def _search_vibe_memory(self, query: str) -> Dict[str, List[Dict[str, Any]]]:
        """Vibe Memory .md íŒŒì¼ë“¤ì—ì„œ ê²€ìƒ‰"""
        results = {}
        query_lower = query.lower()
        
        if not self.memory_bank_path or not os.path.exists(self.memory_bank_path):
            return results
        
        md_files = {
            'coding_flow.md': 'tasks',
            'feature_roadmap.md': 'features',
            'project_vision.md': 'vision',
            'decision-log.md': 'decisions',
            'system-patterns.md': 'patterns'
        }
        
        for filename, category in md_files.items():
            filepath = os.path.join(self.memory_bank_path, filename)
            if os.path.exists(filepath):
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # ì¤„ ë‹¨ìœ„ë¡œ ê²€ìƒ‰
                    lines = content.splitlines()
                    matches = []
                    
                    for i, line in enumerate(lines):
                        if query_lower in line.lower():
                            # ì»¨í…ìŠ¤íŠ¸ í¬í•¨ (ì•ë’¤ 2ì¤„)
                            start = max(0, i - 2)
                            end = min(len(lines), i + 3)
                            context = '\n'.join(lines[start:end])
                            
                            matches.append({
                                'line': i + 1,
                                'text': line.strip(),
                                'context': context,
                                'file': filename,
                                'relevance': self._calculate_relevance(query_lower, line.lower())
                            })
                    
                    if matches:
                        results[category] = matches
                        
                except Exception as e:
                    print(f"  âš ï¸ {filename} ì½ê¸° ì‹¤íŒ¨: {e}")
        
        return results
    
    def _search_project_cache(self, query: str) -> List[Dict[str, Any]]:
        """í”„ë¡œì íŠ¸ ìºì‹œ íŒŒì¼ì—ì„œ ê²€ìƒ‰"""
        results = []
        query_lower = query.lower()
        
        cache_dir = os.path.join(self.project_path, '.cache')
        if not os.path.exists(cache_dir):
            return results
        
        # í”„ë¡œì íŠ¸ ID ê¸°ë°˜ ìºì‹œ íŒŒì¼ë“¤
        project_id = self.project_context.get('project_id')
        if project_id:
            cache_files = [
                f'cache_{project_id}.json',
                f'index_{project_id}.json',
                f'session_{project_id}.json'
            ]
            
            for cache_file in cache_files:
                filepath = os.path.join(cache_dir, cache_file)
                if os.path.exists(filepath):
                    try:
                        with open(filepath, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                        
                        # JSON ë‚´ìš© ê²€ìƒ‰
                        matches = self._search_json_recursive(data, query_lower, cache_file)
                        results.extend(matches)
                        
                    except Exception as e:
                        print(f"  âš ï¸ {cache_file} ì½ê¸° ì‹¤íŒ¨: {e}")
        
        return results
    
    def _search_claude_memory(self, query: str) -> List[Dict[str, Any]]:
        """Claude Memory (ê°œë°œ ê²½í—˜)ì—ì„œ ê²€ìƒ‰"""
        results = []
        query_lower = query.lower()
        
        # project_contextì˜ experiences ê²€ìƒ‰
        experiences = self.project_context.get('experiences', [])
        if hasattr(self.project_context, 'get'):
            experiences.extend(self.project_context.get('coding_experiences', []))
        
        for exp in experiences:
            exp_str = json.dumps(exp, ensure_ascii=False).lower()
            if query_lower in exp_str:
                results.append({
                    'type': 'experience',
                    'task': exp.get('task', 'Unknown'),
                    'category': exp.get('category'),
                    'timestamp': exp.get('timestamp'),
                    'importance': exp.get('importance', 0.5),
                    'data': exp,
                    'relevance': self._calculate_relevance(query_lower, exp_str)
                })
        
        return results
    
    def _search_json_recursive(self, obj: Any, query: str, source: str, path: str = '') -> List[Dict[str, Any]]:
        """JSON ê°ì²´ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ê²€ìƒ‰"""
        results = []
        
        if isinstance(obj, dict):
            for key, value in obj.items():
                new_path = f"{path}.{key}" if path else key
                
                # í‚¤ì—ì„œ ê²€ìƒ‰
                if query in key.lower():
                    results.append({
                        'type': 'json_key',
                        'path': new_path,
                        'key': key,
                        'value': str(value)[:100],
                        'source': source,
                        'relevance': self._calculate_relevance(query, key.lower())
                    })
                
                # ê°’ì—ì„œ ì¬ê·€ ê²€ìƒ‰
                results.extend(self._search_json_recursive(value, query, source, new_path))
                
        elif isinstance(obj, list):
            for i, item in enumerate(obj):
                new_path = f"{path}[{i}]"
                results.extend(self._search_json_recursive(item, query, source, new_path))
                
        elif isinstance(obj, str):
            if query in obj.lower():
                results.append({
                    'type': 'json_value',
                    'path': path,
                    'value': obj[:200],
                    'source': source,
                    'relevance': self._calculate_relevance(query, obj.lower())
                })
        
        return results
    
    def _calculate_relevance(self, query: str, text: str) -> float:
        """ê²€ìƒ‰ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (0~1)"""
        # ì •í™•íˆ ì¼ì¹˜
        if query == text:
            return 1.0
        
        # ë‹¨ì–´ ê²½ê³„ì—ì„œ ì‹œì‘
        if text.startswith(query):
            return 0.9
        
        # ë‹¨ì–´ë¡œ í¬í•¨
        if f' {query} ' in f' {text} ':
            return 0.8
        
        # ë¶€ë¶„ ë¬¸ìì—´ë¡œ í¬í•¨
        if query in text:
            return 0.6
        
        return 0.5
    
    def _rank_and_limit_results(self, results: Dict[str, Any], limit: int) -> Dict[str, Any]:
        """ê²°ê³¼ë¥¼ ê´€ë ¨ì„± ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ì œí•œ"""
        # ëª¨ë“  ê²°ê³¼ë¥¼ í‰ë©´í™”í•˜ì—¬ ì •ë ¬
        all_results = []
        
        for source, source_results in results.get('sources', {}).items():
            if isinstance(source_results, list):
                for item in source_results:
                    item['_source'] = source
                    all_results.append(item)
            elif isinstance(source_results, dict):
                for category, items in source_results.items():
                    for item in items:
                        item['_source'] = f"{source}.{category}"
                        all_results.append(item)
        
        # ê´€ë ¨ì„± ìˆœìœ¼ë¡œ ì •ë ¬
        all_results.sort(key=lambda x: x.get('relevance', 0), reverse=True)
        
        # ìƒìœ„ Nê°œë§Œ ìœ ì§€
        results['ranked_results'] = all_results[:limit]
        
        return results


# ê²€ìƒ‰ í—¬í¼ í•¨ìˆ˜ë“¤
def integrated_search(
    query: str, 
    search_type: str = 'all', 
    limit: int = 20,
    project_context: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """í†µí•© ê²€ìƒ‰ ì‹¤í–‰
    
    Args:
        query: ê²€ìƒ‰ì–´
        search_type: ê²€ìƒ‰ íƒ€ì… ('all', 'code', 'tasks', 'memory', 'cache')
        limit: ìµœëŒ€ ê²°ê³¼ ìˆ˜
        project_context: í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì )
    
    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    # project_context ê°€ì ¸ì˜¤ê¸°
    if project_context is None:
        # globalsì—ì„œ project_context í™•ì¸
        if 'project_context' in globals():
            project_context = globals()['project_context']
        # ë˜ëŠ” global_project_contextë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ì €ì¥ë˜ì–´ ìˆì„ ìˆ˜ë„ ìˆìŒ
        elif 'global_project_context' in globals():
            project_context = globals()['global_project_context']
        else:
            # project_contextë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° ë¹ˆ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
            project_context = {}
    
    engine = IntegratedSearchEngine(project_context)
    return engine.search(query, search_type, limit)


def search_by_date(date_range: str, project_context: Optional[dict] = None) -> Dict[str, Any]:
    """
    ë‚ ì§œ ê¸°ë°˜ ê²€ìƒ‰
    
    Args:
        date_range: ë‚ ì§œ ë²”ìœ„ ë¬¸ìì—´
            - 'today': ì˜¤ëŠ˜
            - 'yesterday': ì–´ì œ
            - 'last_week': ì§€ë‚œ ì£¼
            - 'last_month': ì§€ë‚œ ë‹¬
            - 'YYYY-MM-DD': íŠ¹ì • ë‚ ì§œ
            - 'YYYY-MM-DD to YYYY-MM-DD': ë‚ ì§œ ë²”ìœ„
        project_context: í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì )
        
    Returns:
        ë‚ ì§œë³„ ê²€ìƒ‰ ê²°ê³¼
    """
    # project_context ì´ˆê¸°í™” ë° íƒ€ì… ë³´ì¥
    if project_context is None:
        project_context = globals().get('project_context', {})
    
    # project_contextê°€ ì—¬ì „íˆ Noneì´ê±°ë‚˜ falsyë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì„¤ì •
    if not project_context:
        project_context = {}
    
    # íƒ€ì… ì²´ì»¤ë¥¼ ìœ„í•œ ëª…ì‹œì  í™•ì¸
    assert isinstance(project_context, dict), "project_context must be a dictionary"
    
    # ì´ì œ project_contextëŠ” í™•ì‹¤íˆ dict íƒ€ì…
    
    # ë‚ ì§œ ë²”ìœ„ íŒŒì‹±
    end_date = datetime.now()
    start_date = end_date
    
    if date_range == 'today':
        start_date = end_date.replace(hour=0, minute=0, second=0, microsecond=0)
    elif date_range == 'yesterday':
        start_date = (end_date - timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
        end_date = start_date + timedelta(days=1)
    elif date_range == 'last_week':
        start_date = end_date - timedelta(days=7)
    elif date_range == 'last_month':
        start_date = end_date - timedelta(days=30)
    elif ' to ' in date_range:
        # ë‚ ì§œ ë²”ìœ„ íŒŒì‹±
        try:
            start_str, end_str = date_range.split(' to ')
            start_date = datetime.strptime(start_str.strip(), '%Y-%m-%d')
            end_date = datetime.strptime(end_str.strip(), '%Y-%m-%d')
        except ValueError:
            return {'error': f'Invalid date range format: {date_range}'}
    else:
        # ë‹¨ì¼ ë‚ ì§œ
        try:
            start_date = datetime.strptime(date_range, '%Y-%m-%d')
            end_date = start_date + timedelta(days=1)
        except ValueError:
            return {'error': f'Invalid date format: {date_range}'}
    
    results = {
        'date_range': date_range,
        'start_date': start_date.isoformat(),
        'end_date': end_date.isoformat(),
        'sources': {},
        'total_results': 0
    }
    
    # 1. ì‘ì—… íˆìŠ¤í† ë¦¬ì—ì„œ ê²€ìƒ‰
    work_history = project_context.get('cache', {}).get('work_tracking', {}).get('history', [])
    history_results = []
    
    for entry in work_history:
        entry_time = datetime.fromisoformat(entry.get('timestamp', ''))
        if start_date <= entry_time <= end_date:
            history_results.append(entry)
    
    if history_results:
        results['sources']['work_history'] = history_results
        results['total_results'] += len(history_results)
    
    # 2. ìµœê·¼ ì‘ì—…ì—ì„œ ê²€ìƒ‰
    recent_ops = project_context.get('cache', {}).get('recent_operations', [])
    recent_results = []
    
    for op in recent_ops:
        op_time = datetime.fromisoformat(op.get('timestamp', ''))
        if start_date <= op_time <= end_date:
            recent_results.append(op)
    
    if recent_results:
        results['sources']['recent_operations'] = recent_results
        results['total_results'] += len(recent_results)
    
    # 3. íŒŒì¼ ìˆ˜ì • ì‹œê°„ ê¸°ë°˜ ê²€ìƒ‰
    file_results = []
    project_path = project_context.get('path', '.')
    
    for root, dirs, files in os.walk(project_path):
        # ë°±ì—… ë””ë ‰í† ë¦¬ ì œì™¸
        if 'backup' in root or 'node_modules' in root:
            continue
            
        for file in files:
            if file.endswith(('.py', '.js', '.ts', '.json', '.md')):
                file_path = os.path.join(root, file)
                try:
                    mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
                    if start_date <= mtime <= end_date:
                        file_results.append({
                            'file': os.path.relpath(file_path, project_path),
                            'modified': mtime.isoformat(),
                            'size': os.path.getsize(file_path)
                        })
                except:
                    pass
    
    if file_results:
        # ìˆ˜ì • ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬
        file_results.sort(key=lambda x: x['modified'], reverse=True)
        results['sources']['modified_files'] = file_results
        results['total_results'] += len(file_results)
    
    return results


def search_by_file(file_pattern: str, project_context: Optional[dict] = None) -> Dict[str, Any]:
    """
    íŒŒì¼ íŒ¨í„´ ê¸°ë°˜ ê²€ìƒ‰
    
    Args:
        file_pattern: íŒŒì¼ íŒ¨í„´ ë¬¸ìì—´
            - '*.py': ëª¨ë“  Python íŒŒì¼
            - 'test_*.py': testë¡œ ì‹œì‘í•˜ëŠ” Python íŒŒì¼
            - '**/*.js': ëª¨ë“  í•˜ìœ„ ë””ë ‰í† ë¦¬ì˜ JavaScript íŒŒì¼
            - 'helpers.py': íŠ¹ì • íŒŒì¼ëª…
            - 'src/': íŠ¹ì • ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼ë“¤
        project_context: í”„ë¡œì íŠ¸ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì )
        
    Returns:
        íŒŒì¼ íŒ¨í„´ì— ë§¤ì¹­ë˜ëŠ” ê²€ìƒ‰ ê²°ê³¼
    """
    # project_context ì´ˆê¸°í™” ë° íƒ€ì… ë³´ì¥
    if project_context is None:
        project_context = globals().get('project_context', {})
    
    # project_contextê°€ ì—¬ì „íˆ Noneì´ê±°ë‚˜ falsyë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì„¤ì •
    if not project_context:
        project_context = {}
    
    # íƒ€ì… ì²´ì»¤ë¥¼ ìœ„í•œ ëª…ì‹œì  í™•ì¸
    assert isinstance(project_context, dict), "project_context must be a dictionary"
    
    # ì´ì œ project_contextëŠ” í™•ì‹¤íˆ dict íƒ€ì…
    
    project_path = project_context.get('path', '.')
    
    results = {
        'file_pattern': file_pattern,
        'timestamp': datetime.now().isoformat(),
        'sources': {},
        'total_results': 0
    }
    
    # 1. íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ íŒ¨í„´ ë§¤ì¹­
    import fnmatch
    file_matches = []
    
    # ë””ë ‰í† ë¦¬ íŒ¨í„´ì¸ì§€ í™•ì¸
    is_dir_pattern = file_pattern.endswith('/') or file_pattern.endswith('\\')
    clean_pattern = file_pattern.rstrip('/\\')
    
    for root, dirs, files in os.walk(project_path):
        # ë°±ì—…ê³¼ node_modules ì œì™¸
        if 'backup' in root or 'node_modules' in root or '.git' in root:
            continue
        
        rel_root = os.path.relpath(root, project_path)
        
        # ë””ë ‰í† ë¦¬ íŒ¨í„´ ë§¤ì¹­
        if is_dir_pattern:
            if fnmatch.fnmatch(rel_root, clean_pattern) or rel_root.startswith(clean_pattern):
                # í•´ë‹¹ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼ ì¶”ê°€
                for file in files:
                    if file.endswith(('.py', '.js', '.ts', '.json', '.md')):
                        file_path = os.path.join(root, file)
                        rel_path = os.path.relpath(file_path, project_path)
                        try:
                            file_info = {
                                'path': rel_path,
                                'name': file,
                                'directory': rel_root,
                                'size': os.path.getsize(file_path),
                                'modified': datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat(),
                                'extension': os.path.splitext(file)[1]
                            }
                            file_matches.append(file_info)
                        except:
                            pass
        else:
            # íŒŒì¼ íŒ¨í„´ ë§¤ì¹­
            for file in files:
                file_path = os.path.join(root, file)
                rel_path = os.path.relpath(file_path, project_path)
                
                # ë‹¤ì–‘í•œ íŒ¨í„´ ë§¤ì¹­ ì‹œë„
                if (fnmatch.fnmatch(file, file_pattern) or 
                    fnmatch.fnmatch(rel_path, file_pattern) or
                    fnmatch.fnmatch(rel_path.replace('\\', '/'), file_pattern)):
                    try:
                        file_info = {
                            'path': rel_path,
                            'name': file,
                            'directory': rel_root,
                            'size': os.path.getsize(file_path),
                            'modified': datetime.fromtimestamp(os.path.getmtime(file_path)).isoformat(),
                            'extension': os.path.splitext(file)[1]
                        }
                        file_matches.append(file_info)
                    except:
                        pass
    
    if file_matches:
        # ê²½ë¡œ ê¸¸ì´ë¡œ ì •ë ¬ (ì§§ì€ ê²½ë¡œ ìš°ì„ )
        file_matches.sort(key=lambda x: (len(x['path']), x['path']))
        results['sources']['file_system'] = file_matches
        results['total_results'] += len(file_matches)
    
    # 2. ìºì‹œëœ íŒŒì¼ ì •ë³´ì—ì„œ ê²€ìƒ‰
    cache = project_context.get('cache', {})
    analyzed_files = cache.get('analyzed_files', {})
    cache_matches = []
    
    for cached_path, file_data in analyzed_files.items():
        rel_path = os.path.relpath(cached_path, project_path) if os.path.isabs(cached_path) else cached_path
        
        if (fnmatch.fnmatch(os.path.basename(cached_path), file_pattern) or
            fnmatch.fnmatch(rel_path, file_pattern) or
            fnmatch.fnmatch(rel_path.replace('\\', '/'), file_pattern)):
            
            cache_info = {
                'path': rel_path,
                'cached_at': file_data.get('timestamp', 'unknown'),
                'has_analysis': 'data' in file_data
            }
            
            if isinstance(file_data, dict) and 'data' in file_data:
                analysis = file_data['data']
                cache_info.update({
                    'functions': len(analysis.get('functions', [])),
                    'classes': len(analysis.get('classes', [])),
                    'lines': analysis.get('total_lines', 0)
                })
            
            cache_matches.append(cache_info)
    
    if cache_matches:
        results['sources']['cache'] = cache_matches
        results['total_results'] += len(cache_matches)
    
    # 3. ì‘ì—… ì¶”ì ì—ì„œ ìµœê·¼ ì ‘ê·¼í•œ íŒŒì¼ ê²€ìƒ‰
    work_tracking = cache.get('work_tracking', {})
    file_access = work_tracking.get('file_access', {})
    access_matches = []
    
    for accessed_file, access_info in file_access.items():
        rel_path = os.path.relpath(accessed_file, project_path) if os.path.isabs(accessed_file) else accessed_file
        
        if (fnmatch.fnmatch(os.path.basename(accessed_file), file_pattern) or
            fnmatch.fnmatch(rel_path, file_pattern) or
            fnmatch.fnmatch(rel_path.replace('\\', '/'), file_pattern)):
            
            access_matches.append({
                'path': rel_path,
                'access_count': access_info.get('count', 0),
                'last_accessed': access_info.get('last_access', 'unknown'),
                'operations': access_info.get('operations', [])
            })
    
    if access_matches:
        # ì ‘ê·¼ íšŸìˆ˜ë¡œ ì •ë ¬
        access_matches.sort(key=lambda x: x['access_count'], reverse=True)
        results['sources']['recent_access'] = access_matches
        results['total_results'] += len(access_matches)
    
    # 4. ë©”íƒ€ ì •ë³´ ì¶”ê°€
    if results['total_results'] > 0:
        # í™•ì¥ìë³„ í†µê³„
        extensions = {}
        for source_data in results['sources'].values():
            for item in source_data:
                if 'extension' in item:
                    ext = item['extension']
                    extensions[ext] = extensions.get(ext, 0) + 1
        
        results['statistics'] = {
            'by_extension': extensions,
            'unique_files': len(set(
                item['path'] for source_data in results['sources'].values() 
                for item in source_data if 'path' in item
            ))
        }
    
    return results


# ì „ì—­ ë“±ë¡
if __name__ != "__main__":
    print("âœ… í†µí•© ê²€ìƒ‰ ì‹œìŠ¤í…œ ë¡œë“œë¨")
    print("  - integrated_search(query, type='all', limit=20)")
    print("  - íƒ€ì…: 'all', 'code', 'tasks', 'memory', 'cache'")
    print("  - search_by_date(date_range)")
    print("  - search_by_file(file_pattern)")
