"""
Code Analyzer with O3 Integration
O3 LLMì„ ë‚´ë¶€ì ìœ¼ë¡œ í™œìš©í•˜ëŠ” ê°•í™”ëœ ì½”ë“œ ë¶„ì„ê¸°
"""
import os
import sys
import json
import time
import asyncio
from typing import Dict, List, Any, Optional
from datetime import datetime
from pathlib import Path

# AI Helpers ì„í¬íŠ¸
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
import ai_helpers_new as h

class CodeAnalyzerWithO3:
    """O3 í†µí•© ì½”ë“œ ë¶„ì„ê¸° - ìë™ ë³‘ë ¬ ì‹¤í–‰"""
    
    def __init__(self, auto_o3: bool = True):
        """
        Args:
            auto_o3: O3 ë¶„ì„ ìë™ ì‹¤í–‰ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        """
        self.auto_o3 = auto_o3
        self.analysis_cache = {}
        self.o3_tasks = {}
        self.start_time = None
        
    def analyze(self, file_path: str, deep_analysis: bool = True) -> Dict[str, Any]:
        """
        ë©”ì¸ ë¶„ì„ í•¨ìˆ˜ - O3ì™€ ì •ì  ë¶„ì„ ë³‘ë ¬ ì‹¤í–‰
        
        Args:
            file_path: ë¶„ì„í•  íŒŒì¼ ê²½ë¡œ
            deep_analysis: O3 ì‹¬ì¸µ ë¶„ì„ í¬í•¨ ì—¬ë¶€
            
        Returns:
            í†µí•© ë¶„ì„ ê²°ê³¼
        """
        self.start_time = time.time()
        print(f"ğŸ” ì½”ë“œ ë¶„ì„ ì‹œì‘: {file_path}")
        
        # 1. ì½”ë“œ ì½ê¸°
        code_content = self._read_code(file_path)
        if not code_content:
            return {'error': 'íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'}
        
        # 2. ë³‘ë ¬ ë¶„ì„ ì‹œì‘
        results = {
            'file': file_path,
            'timestamp': datetime.now().isoformat(),
            'analysis_mode': 'parallel_with_o3' if self.auto_o3 else 'static_only'
        }
        
        # 3. ì •ì  ë¶„ì„ (ì¦‰ì‹œ ì‹¤í–‰)
        print("  ğŸ“Š ì •ì  ë¶„ì„ ì‹œì‘...")
        static_analysis = self._perform_static_analysis(code_content, file_path)
        results['static_analysis'] = static_analysis
        
        # 4. O3 ë¶„ì„ (ë¹„ë™ê¸° ì‹¤í–‰)
        if self.auto_o3 and deep_analysis:
            print("  ğŸ§  O3 ì‹¬ì¸µ ë¶„ì„ ì‹œì‘...")
            o3_task_id = self._start_o3_analysis(code_content, static_analysis)
            results['o3_task_id'] = o3_task_id
            
            # 5. O3 ì§„í–‰ ì¤‘ ì¶”ê°€ ë¶„ì„
            print("  âš¡ ë³‘ë ¬ ì²˜ë¦¬ ì¤‘...")
            runtime_analysis = self._perform_runtime_analysis(file_path)
            results['runtime_analysis'] = runtime_analysis
            
            # 6. O3 ê²°ê³¼ ìˆ˜ì§‘ (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
            o3_result = self._wait_for_o3_result(o3_task_id, timeout=30)
            if o3_result:
                results['o3_analysis'] = o3_result
                print("  âœ… O3 ë¶„ì„ ì™„ë£Œ")
            else:
                results['o3_analysis'] = {'status': 'pending', 'message': 'O3 ë¶„ì„ ì§„í–‰ ì¤‘...'}
                print("  â³ O3 ë¶„ì„ ê³„ì† ì§„í–‰ ì¤‘...")
        
        # 7. ê²°ê³¼ í†µí•©
        results['integrated_insights'] = self._integrate_results(results)
        results['execution_time'] = time.time() - self.start_time
        
        print(f"âœ¨ ë¶„ì„ ì™„ë£Œ: {results['execution_time']:.2f}ì´ˆ")
        return results
    
    def _read_code(self, file_path: str) -> Optional[str]:
        """ì½”ë“œ íŒŒì¼ ì½ê¸°"""
        try:
            result = h.read(file_path)
            if result['ok']:
                return result['data']
        except:
            pass
        
        # ëŒ€ì²´ ë°©ë²•
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except:
            return None
    
    def _perform_static_analysis(self, code: str, file_path: str) -> Dict[str, Any]:
        """ì •ì  ì½”ë“œ ë¶„ì„"""
        analysis = {
            'lines_of_code': len(code.split('\n')),
            'character_count': len(code),
            'issues': [],
            'metrics': {}
        }
        
        # 1. ì½”ë“œ íŒ¨í„´ ë¶„ì„
        patterns = {
            'memory_leak': ['global_cache', 'data_buffer.append', 'self.children.append'],
            'performance': ['for i in range.*for j in range.*for k in range', 'sleep\\(', 'while True'],
            'security': ['eval\\(', 'exec\\(', '__import__', 'pickle\\.loads'],
            'concurrency': ['threading\\.Lock', 'asyncio', 'thread_local'],
            'quality': ['except:', 'pass', 'TODO', 'FIXME', 'XXX']
        }
        
        for category, pattern_list in patterns.items():
            for pattern in pattern_list:
                import re
                matches = re.findall(pattern, code, re.IGNORECASE)
                if matches:
                    analysis['issues'].append({
                        'category': category,
                        'pattern': pattern,
                        'occurrences': len(matches),
                        'severity': self._calculate_severity(category, len(matches))
                    })
        
        # 2. ë³µì¡ë„ ê³„ì‚°
        analysis['metrics']['cyclomatic_complexity'] = self._calculate_complexity(code)
        analysis['metrics']['nesting_depth'] = self._calculate_nesting_depth(code)
        analysis['metrics']['function_count'] = code.count('def ')
        analysis['metrics']['class_count'] = code.count('class ')
        
        # 3. ì˜ì¡´ì„± ë¶„ì„
        imports = re.findall(r'^(?:from|import)\s+(\S+)', code, re.MULTILINE)
        analysis['dependencies'] = imports
        
        # 4. ë¬¸ì œì  ìš”ì•½
        analysis['total_issues'] = len(analysis['issues'])
        analysis['critical_issues'] = sum(1 for i in analysis['issues'] if i['severity'] == 'critical')
        
        return analysis
    
    def _start_o3_analysis(self, code: str, static_analysis: Dict) -> str:
        """O3 ì‹¬ì¸µ ë¶„ì„ ì‹œì‘ (ë¹„ë™ê¸°)"""
        
        # ì •ì  ë¶„ì„ ê²°ê³¼ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        issues_summary = "\n".join([
            f"- {issue['category']}: {issue['occurrences']}ê°œ ({issue['severity']})"
            for issue in static_analysis.get('issues', [])
        ])
        
        prompt = f"""
ì´ ì½”ë“œì— ëŒ€í•œ ì‹¬ì¸µ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”. ì •ì  ë¶„ì„ì—ì„œ ë‹¤ìŒ ë¬¸ì œë“¤ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤:

{issues_summary}

ì½”ë“œ ì¼ë¶€:
{code[:1500]}

ë‹¤ìŒ ê´€ì ì—ì„œ ì¶”ê°€ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. ì•„í‚¤í…ì²˜ ìˆ˜ì¤€ì˜ ë¬¸ì œì ê³¼ ê°œì„  ë°©ì•ˆ
2. ë””ìì¸ íŒ¨í„´ ì ìš© ê¸°íšŒ
3. ì„±ëŠ¥ ìµœì í™” êµ¬ì²´ì  ë°©ë²•
4. ë³´ì•ˆ ì·¨ì•½ì  ë° í•´ê²°ì±…
5. í…ŒìŠ¤íŠ¸ ê°€ëŠ¥ì„± ê°œì„  ë°©ì•ˆ
6. ë¦¬íŒ©í† ë§ ìš°ì„ ìˆœìœ„ì™€ ë‹¨ê³„ë³„ ê³„íš

ê° í•­ëª©ì— ëŒ€í•´ êµ¬ì²´ì ì¸ ì½”ë“œ ì˜ˆì‹œì™€ í•¨ê»˜ ì œì‹œí•´ì£¼ì„¸ìš”.
"""
        
        # O3 ë¹„ë™ê¸° ë¶„ì„ ì‹œì‘
        try:
            if hasattr(h, 'ask_o3_async'):
                result = h.ask_o3_async(prompt, reasoning_effort="high")
            elif hasattr(h, 'llm') and hasattr(h.llm, 'ask_async'):
                result = h.llm.ask_async(prompt, reasoning_effort="high")
            else:
                # O3 ì‚¬ìš© ë¶ˆê°€ ì‹œ íƒœìŠ¤í¬ ID ì‹œë®¬ë ˆì´ì…˜
                result = {'ok': True, 'data': f"simulated_o3_task_{datetime.now().strftime('%Y%m%d_%H%M%S')}"}
            
            if result['ok']:
                task_id = result['data']
                self.o3_tasks[task_id] = {
                    'started_at': datetime.now().isoformat(),
                    'status': 'running'
                }
                return task_id
        except Exception as e:
            print(f"  âš ï¸ O3 ë¶„ì„ ì‹œì‘ ì‹¤íŒ¨: {e}")
        
        return None
    
    def _perform_runtime_analysis(self, file_path: str) -> Dict[str, Any]:
        """ëŸ°íƒ€ì„ ë¶„ì„ (O3 ì²˜ë¦¬ ì¤‘ ë³‘ë ¬ ì‹¤í–‰)"""
        runtime_data = {
            'analyzable': False,
            'execution_test': None,
            'import_check': None
        }
        
        try:
            # AST íŒŒì‹± ì‹œë„
            if hasattr(h, 'code') and hasattr(h.code, 'parse'):
                parsed = h.code.parse(file_path)
                if parsed['ok']:
                    runtime_data['analyzable'] = True
                    runtime_data['ast_data'] = {
                        'functions': len(parsed['data'].get('functions', [])),
                        'classes': len(parsed['data'].get('classes', [])),
                        'imports': parsed['data'].get('imports', [])
                    }
        except:
            pass
        
        # ìˆœí™˜ ì°¸ì¡° ì²´í¬
        runtime_data['circular_reference_risk'] = self._check_circular_references(file_path)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš© ì˜ˆì¸¡
        runtime_data['memory_usage_prediction'] = self._predict_memory_usage(file_path)
        
        return runtime_data
    
    def _wait_for_o3_result(self, task_id: str, timeout: int = 30) -> Optional[Dict]:
        """O3 ê²°ê³¼ ëŒ€ê¸° (íƒ€ì„ì•„ì›ƒ í¬í•¨)"""
        if not task_id:
            return None
        
        start_wait = time.time()
        check_interval = 2  # 2ì´ˆë§ˆë‹¤ í™•ì¸
        
        while time.time() - start_wait < timeout:
            try:
                # O3 ìƒíƒœ í™•ì¸
                if hasattr(h, 'check_o3_status'):
                    status = h.check_o3_status(task_id)
                    if status['ok'] and status['data'] == 'completed':
                        # ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
                        if hasattr(h, 'get_o3_result'):
                            result = h.get_o3_result(task_id)
                            if result['ok']:
                                return self._parse_o3_result(result['data'])
                elif hasattr(h, 'llm'):
                    # llm ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì‚¬ìš©
                    if hasattr(h.llm, 'check_status'):
                        status = h.llm.check_status(task_id)
                        if status['ok'] and status['data'] == 'completed':
                            result = h.llm.get_result(task_id)
                            if result['ok']:
                                return self._parse_o3_result(result['data'])
            except:
                pass
            
            time.sleep(check_interval)
        
        # íƒ€ì„ì•„ì›ƒ - ë‚˜ì¤‘ì— ê²°ê³¼ í™•ì¸ ê°€ëŠ¥
        return None
    
    def _parse_o3_result(self, raw_result: str) -> Dict[str, Any]:
        """O3 ê²°ê³¼ íŒŒì‹±"""
        return {
            'raw_insights': raw_result,
            'processed_at': datetime.now().isoformat(),
            'recommendations': self._extract_recommendations(raw_result)
        }
    
    def _extract_recommendations(self, o3_text: str) -> List[Dict]:
        """O3 í…ìŠ¤íŠ¸ì—ì„œ ê¶Œì¥ì‚¬í•­ ì¶”ì¶œ"""
        recommendations = []
        
        # ê°„ë‹¨í•œ íŒŒì‹± (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŒŒì‹± í•„ìš”)
        lines = o3_text.split('\n')
        current_rec = {}
        
        for line in lines:
            if 'ê°œì„ ' in line or 'ìµœì í™”' in line or 'ë¦¬íŒ©í† ë§' in line:
                if current_rec:
                    recommendations.append(current_rec)
                current_rec = {'description': line.strip(), 'details': []}
            elif current_rec and line.strip():
                current_rec['details'].append(line.strip())
        
        if current_rec:
            recommendations.append(current_rec)
        
        return recommendations
    
    def _integrate_results(self, results: Dict) -> Dict[str, Any]:
        """ëª¨ë“  ë¶„ì„ ê²°ê³¼ í†µí•©"""
        integrated = {
            'summary': {},
            'critical_findings': [],
            'optimization_opportunities': [],
            'action_items': []
        }
        
        # ì •ì  ë¶„ì„ ê²°ê³¼ í†µí•©
        if 'static_analysis' in results:
            static = results['static_analysis']
            integrated['summary']['total_issues'] = static.get('total_issues', 0)
            integrated['summary']['critical_issues'] = static.get('critical_issues', 0)
            integrated['summary']['code_metrics'] = static.get('metrics', {})
            
            # ì¤‘ìš” ë°œê²¬ì‚¬í•­
            for issue in static.get('issues', []):
                if issue['severity'] in ['critical', 'high']:
                    integrated['critical_findings'].append({
                        'type': issue['category'],
                        'description': f"{issue['pattern']} íŒ¨í„´ {issue['occurrences']}íšŒ ë°œê²¬",
                        'severity': issue['severity']
                    })
        
        # O3 ë¶„ì„ ê²°ê³¼ í†µí•©
        if 'o3_analysis' in results and results['o3_analysis']:
            o3 = results['o3_analysis']
            for rec in o3.get('recommendations', []):
                integrated['optimization_opportunities'].append(rec)
        
        # ëŸ°íƒ€ì„ ë¶„ì„ ê²°ê³¼ í†µí•©
        if 'runtime_analysis' in results:
            runtime = results['runtime_analysis']
            if runtime.get('circular_reference_risk'):
                integrated['critical_findings'].append({
                    'type': 'architecture',
                    'description': 'ìˆœí™˜ ì°¸ì¡° ìœ„í—˜ ê°ì§€',
                    'severity': 'high'
                })
        
        # ì•¡ì…˜ ì•„ì´í…œ ìƒì„±
        integrated['action_items'] = self._generate_action_items(integrated)
        
        return integrated
    
    def _generate_action_items(self, integrated: Dict) -> List[Dict]:
        """í†µí•© ê²°ê³¼ì—ì„œ ì•¡ì…˜ ì•„ì´í…œ ìƒì„±"""
        action_items = []
        
        # ìš°ì„ ìˆœìœ„ë³„ ì•¡ì…˜ ì•„ì´í…œ
        for finding in integrated.get('critical_findings', []):
            action_items.append({
                'priority': 'immediate',
                'action': f"{finding['type']} ë¬¸ì œ í•´ê²°",
                'description': finding['description'],
                'estimated_effort': '1-2 hours'
            })
        
        for opp in integrated.get('optimization_opportunities', [])[:3]:  # ìƒìœ„ 3ê°œ
            action_items.append({
                'priority': 'high',
                'action': 'ì„±ëŠ¥ ìµœì í™”',
                'description': opp.get('description', 'ìµœì í™” ê¸°íšŒ'),
                'estimated_effort': '2-4 hours'
            })
        
        return action_items
    
    def _calculate_severity(self, category: str, count: int) -> str:
        """ë¬¸ì œ ì‹¬ê°ë„ ê³„ì‚°"""
        severity_map = {
            'security': {'threshold': 1, 'high': 'critical'},
            'memory_leak': {'threshold': 2, 'high': 'critical'},
            'performance': {'threshold': 3, 'high': 'high'},
            'concurrency': {'threshold': 2, 'high': 'high'},
            'quality': {'threshold': 5, 'high': 'medium'}
        }
        
        if category in severity_map:
            if count >= severity_map[category]['threshold']:
                return severity_map[category]['high']
        
        return 'low'
    
    def _calculate_complexity(self, code: str) -> int:
        """ì‚¬ì´í´ë¡œë§¤í‹± ë³µì¡ë„ ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)"""
        complexity = 1  # ê¸°ë³¸ê°’
        
        # ë¶„ê¸°ë¬¸ ì¹´ìš´íŠ¸
        complexity += code.count('if ')
        complexity += code.count('elif ')
        complexity += code.count('for ')
        complexity += code.count('while ')
        complexity += code.count('except')
        complexity += code.count('case ')  # Python 3.10+
        
        return complexity
    
    def _calculate_nesting_depth(self, code: str) -> int:
        """ìµœëŒ€ ì¤‘ì²© ê¹Šì´ ê³„ì‚°"""
        max_depth = 0
        current_depth = 0
        
        for line in code.split('\n'):
            # ë“¤ì—¬ì“°ê¸° ë ˆë²¨ ê³„ì‚°
            indent = len(line) - len(line.lstrip())
            depth = indent // 4  # 4 spaces = 1 level
            
            if depth > max_depth:
                max_depth = depth
        
        return max_depth
    
    def _check_circular_references(self, file_path: str) -> bool:
        """ìˆœí™˜ ì°¸ì¡° ìœ„í—˜ ì²´í¬"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                code = f.read()
                
            # ê°„ë‹¨í•œ íŒ¨í„´ ì²´í¬
            patterns = [
                'self.parent = self',
                'self.children.append(self)',
                '.parent = .*\n.*\.children.append'
            ]
            
            import re
            for pattern in patterns:
                if re.search(pattern, code):
                    return True
        except:
            pass
        
        return False
    
    def _predict_memory_usage(self, file_path: str) -> str:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                code = f.read()
            
            # ë©”ëª¨ë¦¬ ì§‘ì•½ì  íŒ¨í„´ ì²´í¬
            if 'global_cache' in code and 'clear()' not in code:
                return 'high_risk'
            elif any(pattern in code for pattern in ['data_buffer.append', 'list.append', 'dict[']):
                return 'medium_risk'
            else:
                return 'low_risk'
        except:
            return 'unknown'


# ì‚¬ìš© ì˜ˆì‹œ
def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    analyzer = CodeAnalyzerWithO3(auto_o3=True)
    
    # í…ŒìŠ¤íŠ¸ íŒŒì¼ ë¶„ì„
    test_file = "test_complex_code.py"
    if os.path.exists(test_file):
        print("="*60)
        print("ğŸš€ O3 í†µí•© ì½”ë“œ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸")
        print("="*60)
        
        results = analyzer.analyze(test_file, deep_analysis=True)
        
        # ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“Š ë¶„ì„ ê²°ê³¼:")
        print(f"  â€¢ ì´ ë¬¸ì œì : {results['integrated_insights']['summary'].get('total_issues', 0)}")
        print(f"  â€¢ ì¤‘ìš” ë¬¸ì œ: {results['integrated_insights']['summary'].get('critical_issues', 0)}")
        print(f"  â€¢ ì‹¤í–‰ ì‹œê°„: {results['execution_time']:.2f}ì´ˆ")
        
        print("\nğŸ”´ ì¤‘ìš” ë°œê²¬ì‚¬í•­:")
        for finding in results['integrated_insights']['critical_findings'][:3]:
            print(f"  â€¢ [{finding['severity']}] {finding['description']}")
        
        print("\nâœ… ê¶Œì¥ ì¡°ì¹˜:")
        for item in results['integrated_insights']['action_items'][:3]:
            print(f"  â€¢ [{item['priority']}] {item['action']}: {item['description']}")
        
        print("\n" + "="*60)
        
        # ê²°ê³¼ ì €ì¥
        output_file = "analysis_result.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ ì „ì²´ ê²°ê³¼ ì €ì¥: {output_file}")


if __name__ == "__main__":
    main()